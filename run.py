#-*- coding: utf-8 -*-
"""On Robustness of NODEs Implementation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1goqxuNQp0gTyK39TG6JxM4l-GjtIehqT

!pip install tqdm
!pip install torchdiffeq
!pip install numpy
#!pip install --upgrade torch torchvision
!pip install matplotlib
"""

# Necessary
import os
import sys
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import TensorDataset, DataLoader, random_split
import torchvision
#import matplotlib.pyplot as plt
from tqdm import tqdm
from torchdiffeq import odeint_adjoint as odeint
import argparse



parser = argparse.ArgumentParser()
parser.add_argument("-d", "--device", type=str, default="cpu", help="Device which the PyTorch run on")
parser.add_argument("-bs", "--batch-size", type=int, default=1024, help="Batch size of 1 iteration")
parser.add_argument("-ep", "--epochs", type=int, default=100, help="Numbers of epoch")
parser.add_argument("-f", "--folder", type=str, default="./data/mnist", help="Folder /path/to/mnist/dataset")
parser.add_argument("-r", "--result", type=str, default="./result", help="Folder where the result going in")
parser.add_argument("-tr", "--train", type=int, default=8000, help="Number of train images")
parser.add_argument("-vl", "--valid", type=int, default=2000, help="Number of validation images")
parser.add_argument("-lr", "--learning-rate",type=float, default=1e-3, help="Learning rate in optimizer")
args = parser.parse_args()


 # CONSTANT 
device = args.device
#torch.device("cuda")
EPOCHS=args.epochs
BATCH_SIZE=args.batch_size
DATA_DIR = args.folder
RESULT_DIR = args.result
TRAIN_NUM=args.train
VALID_NUM=args.valid
TEST_NUM=60000-TRAIN_NUM-VALID_NUM
DATA_DISTRIBUTION=[TRAIN_NUM,VALID_NUM,TEST_NUM]

print(device, EPOCHS,BATCH_SIZE,DATA_DIR,RESULT_DIR)
# Data incoming 
#cifar = torchvision.datasets.CIFAR10("./data/")
#data_loader = torch.utils.data.DataLoader(cifar, batch_size=BATCH_SIZE, shuffle=True)

"""![image.png](attachment:image.png)
Conv(in_channels, out_channels, kernel_size, stride)
![image-2.png](attachment:image-2.png)
"""













class ODEBlock(nn.Module):
    def __init__(self):
        super(ODEBlock,self).__init__()
        self.block = nn.Sequential(*[
            nn.Conv2d(64,64,3,1, padding=1),
            nn.GroupNorm(4,64),
            nn.ReLU(), 
            nn.Conv2d(64,64,3,1, padding=1),
            nn.GroupNorm(4,64),
            nn.ReLU()
        ]) 
    def forward(self,t,x):
        return self.block(x)
    #pass
     
class ODENet(nn.Module):
    def __init__(self, func, device="cpu"):
        super(ODENet, self).__init__()
        assert isinstance(func, ODEBlock), f"argument function is not NeuralODEs model"
        self.fe = nn.Sequential(*[nn.Conv2d(1,64,3,1),
                                  nn.GroupNorm(4,64),
                                  nn.ReLU(),
                                  nn.Conv2d(64,64,4,2),
                                  nn.GroupNorm(4,64),
                                  #1x64x12x12
                                  nn.ReLU()])
        self.rm = func
        self.fcc = nn.Sequential(*[nn.Conv2d(64,1,3,1,padding=1),
                                   nn.AdaptiveAvgPool2d(8),
                                   # 1 x 64 x 8 x 8
                                   nn.Flatten(),
                                   # 4096
                                   nn.Linear(64,10),
                                   nn.Softmax()])
        self.intergrated_time = torch.Tensor([0.,1.]).float().to(device)
    def forward(self,x):
        out = self.fe(x)
        out = odeint(self.rm, out, self.intergrated_time)[1]
        #out = self.rm(out)
        out = self.fcc(out)
        return out
    def evaluate(self, test_loader):
        correct = 0
        total = 0 
        running_loss = 0
        
        with torch.no_grad():
            for test_data in test_loader:
                data, label = test_data
                outputs = self.forward(data)
                _, correct_labels = torch.max(label, 1) 
                _, predicted = torch.max(outputs.data, 1)
                total += label.size(0)
                correct += (predicted == correct_labels).sum().item()
                running_loss += F.torch.nn.functional.binary_cross_entropy_with_logits(
                    outputs.float(), label.float()).item()
        acc = round(correct/toral * 1.0 , 2)
        
        return running_loss,acc

class Network(nn.Module):
    def __init__(self):
        super(Network, self).__init__()
        self.net = nn.Sequential(*[
            nn.Conv2d(1,64,3,1),
            nn.GroupNorm(4,64),
            nn.ReLU(),
            nn.Conv2d(64,64,4,2),
            nn.GroupNorm(4,64),
            nn.ReLU(),
            nn.Conv2d(64,64,3,1, padding=1),
            nn.GroupNorm(4,64),
            nn.ReLU(), 
            nn.Conv2d(64,64,3,1, padding=1),
            nn.GroupNorm(4,64),
            nn.ReLU(),
            nn.Conv2d(64,1,3,1,padding=1),
            nn.AdaptiveAvgPool2d(8),
            nn.Flatten(),
            nn.Linear(64,10),
            nn.Softmax()
        ])
    def forward(self,x):
        return self.net(x)
    def evaluate(self, test_loader):
        correct = 0
        total = 0 
        running_loss = 0
        
        with torch.no_grad():
            for test_data in test_loader:
                data, label = test_data
                outputs = self.forward(data)
                _, correct_labels = torch.max(label, 1) 
                _, predicted = torch.max(outputs.data, 1)
                total += label.size(0)
                correct += (predicted == correct_labels).sum().item()
                running_loss += F.torch.nn.functional.binary_cross_entropy_with_logits(
                    outputs.float(), label.float()).item()
        acc = round(correct/toral * 1.0 , 2)
        
        return running_loss,acc

def add_noise(converted_data, sigma = 10,device="cpu"):
    pertubed_data = converted_data + torch.normal(torch.zeros(converted_data.shape),
                                                  torch.ones(converted_data.shape) * sigma).to(device)
    return pertubed_data
def preprocess_data(data, shape = (28,28), device="cpu"):
    X = []
    Y = []
    for data_idx, (x,y) in list(enumerate(data)):
        X.append(np.array(x).reshape((1,shape[0],shape[0])))
        Y.append(y)
    y_data = F.one_hot(torch.Tensor(Y).to(torch.int64), num_classes=10)
    y_data = y_data.to(device)
    x_data = torch.Tensor(X)
    x_data = x_data.to(device)
    ds = TensorDataset(x_data,y_data)
    x_noise_data = add_noise(x_data, device=device)
    pertubed_ds = TensorDataset(x_noise_data,y_data)
    ds_len = len(Y)
    return ds_len, ds, pertubed_ds
# Adversarial Attack chua code ne`, nho code vo, xin cam on

def train_model(model, optimizer, train_loader, val_loader,loss_fn, epochs=100):
    print(model.eval())
    print(f"Numbers of parameters in model: {sum(p.numel() for p in model.parameters() if p.requires_grad)}")
    history = {"loss": [], "acc": [], "val_loss": [], "val_acc": []}
    for epoch_id in tqdm(range(epochs)):
        total = 0
        correct = 0
        running_loss = 0
        print(f"Start epoch number: {epoch_id + 1}")
        for batch_id, data in enumerate(train_loader, 0):
            # get the inputs; data is a list of [inputs, labels]
            print(f"Start batch number: {batch_id + 1} in epoch number: {epoch_id + 1}")
            inputs, labels = data
            print(f"Get data done")
            # zero the parameter gradients
            optimizer.zero_grad()
            print(f"Reset the optimizer backward, grad to 0") 
            # forward + backward + optimize
            outputs = model(inputs)
            print(f"forward data through model")
            _, predicted = torch.max(outputs, 1)
            print(f"Get predicted class")
            _, correct_labels = torch.max(labels, 1)
            print(f"Get label class")
            #print(labels)
            total += labels.size(0)
            correct += (predicted == correct_labels).sum().item()
            print("Calculate the number of correct predictions")
            #print(labels.shape, outputs.shape)
            loss = loss_fn(outputs.float(), labels.float())
            loss.backward()
            print("Backward loss")
            optimizer.step()
            print("Step")
            running_loss += loss.item() 
            print(f"End batch number: {batch_id + 1} in epoch number {epoch_id + 1}")
        acc = round(correct/total * 1.0, 2)
        history["acc"].append(acc)
        history["loss"].append(loss)
        val_loss, val_acc = model.evaluate(val_loader)
        history["val_loss"].append(val_loss)
        history["val_acc"].append(val_acc)
        print(f"Epoch(s) {epoch_id + 1} | loss: {loss} | acc: {acc} | val_loss: {val_loss} | val_acc: {val_acc}")
    return model, history




def main(ds_len, ds, name = "mnist_normal",batch_size=32,epochs=100, lr=1e-3,data_dis=[8000,2000,50000], device="cpu"):
    print(f"Number of train: {data_dis[0]}\nNumber of validation: {data_dis[1]}")
    train_set, val_set, _ = torch.utils.data.random_split(ds,data_dis)
    #print(type(train_set))
    assert isinstance(train_set,torch.utils.data.Dataset)
    train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)
    val_loader = DataLoader(val_set, shuffle=True)
    loss_fn = torch.nn.functional.binary_cross_entropy_with_logits
    ode_func = ODEBlock()
    ode_model = ODENet(ode_func).to(device)
    ode_optimizer = torch.optim.Adam(ode_model.parameters(), lr=lr)
    dnn_model = Network().to(device)
    dnn_optimizer = torch.optim.Adam(dnn_model.parameters(), lr=lr)
    ode_his = train_model(ode_model,
                          ode_optimizer,
                          train_loader, val_loader, loss_fn=loss_fn, epochs=1)
    dnn_his = train_model(dnn_model, 
                         dnn_optimizer,
                         train_loader, val_loader, loss_fn=loss_fn,epochs=1)

# Test   
#test_input = torch.rand((1,1,28,28))
#ode_block = ODEBlock()
#model = ODENet(ode_block)
#model(test_input).shape
#Test OK

# Load data
MNIST = torchvision.datasets.MNIST(DATA_DIR,
                                   train=True,
                                   transform=None,
                                   target_transform=None, download=True)

ds_len_, normal_ds_, pertubed_ds_ = preprocess_data(MNIST, device=device)

main(ds_len_,normal_ds_, device=device, batch_size=BATCH_SIZE, epochs=1, data_dis=DATA_DISTRIBUTION)



